## ðŸ³ Why We Need Kubernetes (Docker Problem Example)

- Suppose there are **100 containers running on one Docker host** (1, 2, 3â€¦100).  
- **One container uses too many resources (CPU/Memory)** compared to others.  
- Because of this:  
  - Some containers may **crash (die)** as they donâ€™t get enough resources.  
  - There may not be enough resources to **create new containers**.  
- **All containers are on the same single host**, so resource sharing becomes a big problem.  
- You need to:  
  - **Manually check** if all containers are running.  
  - **Manually recreate** any container that has stopped.  
- **Problem:** Docker has **no auto-healing**. If a container dies, Docker will NOT restart it automatically.  

---

## âœ… Solution â€“ Kubernetes

- **Auto-healing** â†’ If a container (pod) dies, Kubernetes automatically creates a new one.  
- **No manual monitoring needed** â†’ Kubernetes takes care of container health.  
_______________________

# **âŒ Problems with Docker**

1. **Single Host Limitation** â€“ Docker normally runs on one server only. Managing containers across many servers is difficult.

2. **No Auto-healing** â€“ If a container crashes, Docker will not restart it automatically.

3. **No Auto-scaling** â€“ If traffic increases, Docker will not create extra containers automatically.

4. **Manual Load Balancing** â€“ You need to set up load balancing manually.

5. **Weak Resource Management** â€“ If one container uses too many resources, other containers may slow down or even crash.

6. **Rolling Updates are Hard** â€“ Deploying new updates without downtime is difficult and manual.

---

## **âœ… How Kubernetes Solves These Problems**

1. **Auto-healing** â€“ If a container dies, Kubernetes will automatically create a new one.

2. **Auto-scaling** â€“ If traffic increases, Kubernetes will automatically add more containers. When traffic goes down, it will reduce them.

3. **Multiple Node Support** â€“ Kubernetes can manage containers running on many servers from a single control system.

4. **Built-in Load Balancing** â€“ Kubernetes automatically distributes traffic to healthy containers.

5. **Better Resource Management** â€“ Kubernetes ensures that containers get proper CPU and memory, so one container cannot affect others too much.

6. **Easy Rolling Updates** â€“ Kubernetes can update applications with zero or very less time.





Kubernetes architecture operates on a master-worker node model, designed for orchestrating containerized applications.

1. Control Plane (Master Node):

The control plane, typically residing on a master node, manages the overall cluster and its state.Â Key components include:

kube-apiserver:

The central management entity, exposing the Kubernetes API for communication with other components and external users.

etcd:

A distributed key-value store that stores the cluster's configuration data and state.

kube-scheduler:

Responsible for selecting the optimal node for newly created Pods based on resource requirements, constraints, and policies.

kube-controller-manager:

Runs various controllers that regulate the cluster's state, such as the Replication Controller (ensuring desired number of Pod replicas) and Endpoint Controller (managing network endpoints).

2. Worker Nodes:

Worker nodes are the machines where your containerized applications run within Pods.Â Each worker node runs:Â 

kubelet:

An agent that communicates with the control plane, ensuring containers are running in Pods as specified in the PodSpecs.Â It manages Pods and their containers on the node.

kube-proxy:

A network proxy that maintains network rules on the host, enabling network communication within and outside the cluster by handling service discovery and load balancing for Pods.

Container Runtime:

Software like Docker, containerd, or CRI-O responsible for running and managing containers on the node.

DEPLOYMENT >>>

+-----------------------------+
|           DEPLOYMENT        
|                             |
|   +-----+  +-----+  +-----+ |
|   | Pod |  | Pod |  | Pod | |
|   +-----+  +-----+  +-----+ |
|   +-----+  +-----+  +-----+ |
|   | Pod |  | Pod |  | Pod | |
|   +-----+  +-----+  +-----+ |
|   +-----+  +-----+  +-----+ |
|   | Pod |  | Pod |  | Pod | |
|   +-----+  +-----+  +-----+ |
+-----------------------------+
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80


persistent volume and persistent volume claim

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nginx-pv
  labels:
    env: dev
spec:
  storageClassName: standard
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/home/ubuntu/data"

------------------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: nginx-service-np
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 80
      nodePort: 31285



